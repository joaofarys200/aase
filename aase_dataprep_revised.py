# -*- coding: utf-8 -*-
"""AASE_DATAPREP_revised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k3FP3hsJz60-8QTia_3iOe1g3_HuJQPX

# Data Preparation — CRISP-DM (revisado)\n\nEste notebook contém a pipeline de *Data Preparation* organizada segundo o CRISP-DM: **Clean → Construct → Integrate → Format**. Mantém cópias _antes_ e _depois_ das transformações para comparação visual e estatística.
"""

# 0) Carregar bibliotecas e dataset
import pandas as pd
import numpy as np
import matplotlib

# Usar backend não interativo para não abrir janelas de gráficos
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import seaborn as sns
from joblib import dump
from sqlalchemy import create_engine

from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree
from sklearn.metrics import (
    mean_absolute_error,
    mean_squared_error,
    r2_score,
    make_scorer,
    f1_score,
    precision_score,
    recall_score,
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score,
)
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from xgboost import XGBClassifier
sns.set(style='whitegrid')

# Ajusta o caminho se necessário
DATA_PATH = "ScreenTime vs MentalWellness.csv"

df = pd.read_csv(DATA_PATH)
print("Dados carregados:", df.shape)
df.head()

"""Seção 1) Clean Data: remoção de colunas irrelevantes."""

# Copiar original
df_original = df.copy()

# Identificar colunas Unnamed e user_id
cols_to_drop = [c for c in df.columns if c.lower().startswith('unnamed')] + (['user_id'] if 'user_id' in df.columns else [])
cols_to_drop = [c for c in cols_to_drop if c in df.columns]

print("Colunas a remover:", cols_to_drop)

# Criar df_before (remoção apenas lógica para comparação visual)
df_before = df_original.drop(columns=cols_to_drop, errors='ignore')

# Aplicar remoção no dataset que será preparado
df_clean = df_original.drop(columns=cols_to_drop, errors='ignore').copy()

# Mostrar sumário de nulos e tipos
print('\nResumo de tipos e nulos (df_clean):')
print(df_clean.info())
print(df_clean.isna().sum())

"""Seção 2) Limpeza de outliers (IQR)."""

target = 'mental_wellness_index_0_100'

if target in df_clean.columns:
    # Calcular limites IQR apenas para a variável target
    Q1 = df_clean[target].quantile(0.25)
    Q3 = df_clean[target].quantile(0.75)
    IQR = Q3 - Q1

    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR

    # Criar dois datasets:
    # 1️⃣ Cenário 1 — com outliers
    df_with_outliers = df_clean.copy()

    # 2️⃣ Cenário 2 — sem outliers
    df_no_outliers = df_clean[(df_clean[target] >= lower) & (df_clean[target] <= upper)].copy()

    print(f"Outliers removidos no target '{target}': {df_clean.shape[0] - df_no_outliers.shape[0]}")
    print(f"Cenário 1 (com outliers): {df_with_outliers.shape}")
    print(f"Cenário 2 (sem outliers): {df_no_outliers.shape}")
else:
    print(f"A variável alvo '{target}' não foi encontrada no dataset.")

df_clean.describe()

df_no_outliers.describe()

"""Seção 3) Comparação visual antes vs depois da remoção de outliers."""

# Variáveis para comparar (ajusta conforme necessário)
vars_to_plot = [
    'mental_wellness_index_0_100','productivity_0_100','screen_time_hours','sleep_hours',
    'stress_level_0_10','leisure_screen_hours','work_screen_hours','exercise_minutes_per_week',
    'sleep_quality_1_5'
]
vars_to_plot = [v for v in vars_to_plot if v in df_before.columns and v in df_no_outliers.columns]
print('Variáveis a plotar:', vars_to_plot)

def plot_before_after(var):
    fig, axes = plt.subplots(2,2, figsize=(12,6), gridspec_kw={'height_ratios':[3,1]})
    fig.suptitle(f"{var} — Antes vs Depois", fontsize=14)
    sns.histplot(df_before[var].dropna(), bins=25, ax=axes[0,0])
    axes[0,0].set_title('Antes (histograma)')
    sns.histplot(df_no_outliers[var].dropna(), bins=25, ax=axes[0,1], color='tab:green')
    axes[0,1].set_title('Depois (histograma)')
    sns.boxplot(x=df_before[var].dropna(), ax=axes[1,0])
    axes[1,0].set_title('Antes (boxplot)')
    sns.boxplot(x=df_no_outliers[var].dropna(), ax=axes[1,1], color='lightgreen')
    axes[1,1].set_title('Depois (boxplot)')
    plt.tight_layout(rect=[0,0.03,1,0.95])
    plt.show()

for v in vars_to_plot:
    plot_before_after(v)

"""Seção 4) Construct Data — criação de features derivadas."""

# Começar a partir do df_no_outliers
df_feat = df_no_outliers.copy()

# Exemplos de features derivadas (ajusta conforme insights da tua análise)
if 'screen_time_hours' in df_feat.columns and 'stress_level_0_10' in df_feat.columns:
    df_feat['screen_stress_index'] = df_feat['screen_time_hours'] * df_feat['stress_level_0_10']

if 'sleep_hours' in df_feat.columns and 'sleep_quality_1_5' in df_feat.columns:
    df_feat['sleep_score'] = df_feat['sleep_hours'] * df_feat['sleep_quality_1_5']

if 'exercise_minutes_per_week' in df_feat.columns:
    df_feat['exercise_hours_per_day'] = (df_feat['exercise_minutes_per_week'] / 60) / 7

if 'leisure_screen_hours' in df_feat.columns and 'work_screen_hours' in df_feat.columns:
    df_feat['leisure_work_ratio'] = df_feat['leisure_screen_hours'] / (df_feat['work_screen_hours'] + 0.01)

print('Novas features adicionadas. Shape atual:', df_feat.shape)
df_feat.head()

target = 'mental_wellness_index_0_100'
if target in df_no_outliers.columns and target in df_clean.columns:
    # Selecionar features contínuas mais relevantes
    candidates = ['screen_time_hours', 'stress_level_0_10', 'sleep_hours',
                  'exercise_minutes_per_week', 'leisure_work_ratio']
    pairs = [(x, target) for x in candidates if x in df_no_outliers.columns]

    cols_per_row = 2
    rows = (len(pairs) + cols_per_row - 1) // cols_per_row

    fig, axes = plt.subplots(rows, cols_per_row, figsize=(14, 4 * rows))
    axes = axes.flatten()

    for i, (x, y) in enumerate(pairs):
        sns.regplot(data=df_no_outliers, x=x, y=y,
                    scatter_kws={'alpha':0.4}, line_kws={'color':'red'}, ax=axes[i])
        axes[i].set_title(f"Antes: {y} vs {x}", fontsize=12)
        sns.regplot(data=df_clean, x=x, y=y,
                    scatter_kws={'alpha':0.4}, line_kws={'color':'green'}, ax=axes[i])

    for j in range(i+1, len(axes)):
        axes[j].set_visible(False)

    plt.tight_layout()
    plt.show()
else:
    print("A variável alvo 'mental_wellness_index_0_100' não foi encontrada em ambos os datasets.")

"""Seção 5) Format Data — codificação, normalização e preparação de X/y."""

# 5.1 Identificar categóricas
cat_cols = df_feat.select_dtypes(include=['object', 'category']).columns.tolist()
print('Colunas categóricas a codificar:', cat_cols)

# 5.2 One-Hot Encoding
df_model = pd.get_dummies(df_feat, columns=cat_cols, drop_first=True)
print('Após get_dummies — shape:', df_model.shape)

# 5.3 Converter booleanos (True/False) para 0/1 (int)
bool_cols = df_model.select_dtypes(include='bool').columns.tolist()
if len(bool_cols) > 0:
    df_model[bool_cols] = df_model[bool_cols].astype(int)
    print('Booleanos convertidos para int:', bool_cols)
else:
    print('Nenhuma coluna booleana encontrada.')

# 5.4 Imputação simples (se necessário)
if df_model.isna().sum().sum() > 0:
    print('Existem nulos — a imputação por mediana será aplicada nas numéricas.')
    num_cols_model = df_model.select_dtypes(include=[np.number]).columns.tolist()
    for c in num_cols_model:
        df_model[c].fillna(df_model[c].median(), inplace=True)

# 5.5 Escalar numéricas (excluir targets)
num_cols_model = df_model.select_dtypes(include=[np.number]).columns.tolist()
targets = [c for c in ['mental_wellness_index_0_100','productivity_0_100'] if c in df_model.columns]
features_to_scale = [c for c in num_cols_model if c not in targets]

scaler = StandardScaler()
if len(features_to_scale) > 0:
    df_model[features_to_scale] = scaler.fit_transform(df_model[features_to_scale])

print('Dataset pronto para modelagem — shape:', df_model.shape)

df_model.head()

df_model.describe()

def plot_corr_heatmaps(df1, df2, title1="Antes da formatação", title2="Depois da formatação"):
    common_cols = list(set(df1.select_dtypes(include=[np.number]).columns)
                       .intersection(df2.select_dtypes(include=[np.number]).columns))
    if len(common_cols) < 2:
        print("Não há colunas numéricas suficientes para comparar.")
        return

    corr1 = df1[common_cols].corr(method='pearson')
    corr2 = df2[common_cols].corr(method='pearson')

    fig, axes = plt.subplots(1, 2, figsize=(20, 10))
    sns.heatmap(corr1, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[0], annot=True, square=True)
    axes[0].set_title(title1, fontsize=13)

    sns.heatmap(corr2, cmap='coolwarm', vmin=-1, vmax=1, ax=axes[1], annot=True, square=True)
    axes[1].set_title(title2, fontsize=13)

    plt.tight_layout()
    plt.show()


# Executar a comparação
plot_corr_heatmaps(df_no_outliers, df,
                   title1="Correlação (antes da formatação)",
                   title2="Correlação (depois da formatação)")

num_df = df_model.select_dtypes(include=[np.number])
corr = num_df.corr(method='pearson')

plt.figure(figsize=(18,14))
sns.heatmap(corr, cmap='coolwarm', vmin=-1, vmax=1, cbar_kws={'shrink':0.6}, annot=True)
plt.title('Heatmap final — correlação entre todas as variáveis (após formatação)', fontsize=16)
plt.tight_layout()
plt.show()


if 'mental_wellness_index_0_100' in corr.columns:
    print(corr['mental_wellness_index_0_100'].sort_values(ascending=False).head(10))
else:
    print('Target não encontrada no df_model.')

"""Secção de cross-validation para modelos de regressão."""

def prepare_regression_data(df, target):
    """
    Minimal preparation: Assumes df is already formatted (numerical, no irrelevant cols).
    Just splits into X (features) and y (target).
    """
    if target not in df.columns:
        raise ValueError(f"Target '{target}' not found in DataFrame.")

    y = df[target]
    X = df.drop(columns=[target])

    return X, y

def run_decision_tree_cv(df, target='mental_wellness_index_0_100',
                         title="Decision Tree CV", k=10):
    # Preparar dados
    X, y = prepare_regression_data(df, target)

    # Modelo
    model = DecisionTreeRegressor(
        max_depth=6,
        min_samples_leaf=5,
        random_state=42
    )

    # defenir split neste caso em cima com 10
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)
    rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),
                              greater_is_better=False)
    r2_scorer  = make_scorer(r2_score)

    # Medidas importantes pedidas pelo prof
    mae_scores = -cross_val_score(model, X, y, cv=kf, scoring=mae_scorer)
    rmse_scores = -cross_val_score(model, X, y, cv=kf, scoring=rmse_scorer)
    r2_scores = cross_val_score(model, X, y, cv=kf, scoring=r2_scorer)

    print(f"MAE  → mean={mae_scores.mean():.4f}, std={mae_scores.std():.4f}")
    print(f"RMSE → mean={rmse_scores.mean():.4f}, std={rmse_scores.std():.4f}")
    print(f"R²   → mean={r2_scores.mean():.4f}, std={r2_scores.std():.4f}")


    model.fit(X, y)

    # IMPORTÂNCIA DAS FEATURES
    importances = pd.Series(model.feature_importances_, index=X.columns)
    importances = importances.sort_values(ascending=False).head(12)

    plt.figure(figsize=(10,6))
    sns.barplot(x=importances.values, y=importances.index)
    plt.title(f"Feature Importances — {title}")
    plt.xlabel("Importância")
    plt.tight_layout()
    plt.show()

    plt.figure(figsize=(18,10))
    plot_tree(model, feature_names=X.columns, filled=True, fontsize=6)
    plt.title(f"Decision Tree Structure — {title}")
    plt.show()

    return {
        "MAE_mean": mae_scores.mean(),
        "RMSE_mean": rmse_scores.mean(),
        "R2_mean": r2_scores.mean(),
        "model": model,
        "importances": importances
    }

run_decision_tree_cv(num_df, target='mental_wellness_index_0_100', title='Formatado')

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def run_random_forest_regressor_cv(df, target='mental_wellness_index_0_100',
                                   title="RandomForestRegressor CV", k=10):
    # Preparar dados
    X, y = prepare_regression_data(df, target)

    # Modelo RandomForestRegressor com parâmetros padrão (sem otimização explícita de hiperparâmetros)
    model = RandomForestRegressor(
        n_estimators=100,  # Número padrão de árvores
        random_state=42
    )

    # Definir split para validação cruzada
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    # Definir os scorers para as métricas de regressão
    mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)
    rmse_scorer = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)),
                              greater_is_better=False)
    r2_scorer  = make_scorer(r2_score)

    # Calcular as métricas usando cross_val_score
    mae_scores = -cross_val_score(model, X, y, cv=kf, scoring=mae_scorer)
    rmse_scores = -cross_val_score(model, X, y, cv=kf, scoring=rmse_scorer)
    r2_scores = cross_val_score(model, X, y, cv=kf, scoring=r2_scorer)

    print(f"Resultados de Cross-Validation para {title}:")
    print(f"MAE  \u2192 mean={mae_scores.mean():.4f}, std={mae_scores.std():.4f}")
    print(f"RMSE \u2192 mean={rmse_scores.mean():.4f}, std={rmse_scores.std():.4f}")
    print(f"R\u00b2   \u2192 mean={r2_scores.mean():.4f}, std={r2_scores.std():.4f}")

    # Fit the model on the entire dataset to get feature importances
    model.fit(X, y)

    # IMPORTÂNCIA DAS FEATURES
    importances = pd.Series(model.feature_importances_, index=X.columns)
    importances = importances.sort_values(ascending=False).head(12)

    plt.figure(figsize=(10,6))
    sns.barplot(x=importances.values, y=importances.index, hue=importances.index, palette='viridis', legend=False)
    plt.title(f"Feature Importances \u2014 {title}")
    plt.xlabel("Importância")
    plt.ylabel("Variável")
    plt.tight_layout()
    plt.show()

    return {
        "MAE_mean": mae_scores.mean(),
        "RMSE_mean": rmse_scores.mean(),
        "R2_mean": r2_scores.mean(),
        "model": model,
        "importances": importances
    }

run_random_forest_regressor_cv(num_df, target='mental_wellness_index_0_100', title='RandomForestRegressor - Default Params')

import pandas as pd
from sklearn.model_selection import KFold, cross_val_score
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer
import numpy as np

# As previously, df_model is the final preprocessed dataframe.
# target is 'mental_wellness_index_0_100'

# Separar as features (X) e o target (y) para regressão
# Garante que 'mental_wellness_index_0_100' não é 'productivity_0_100' - este é o target original

if 'mental_wellness_index_0_100' in df_model.columns:
    X_regression = df_model.drop(columns=['mental_wellness_index_0_100'])
    y_regression = df_model['mental_wellness_index_0_100']
    print(f"Dados para regressão preparados. X_regression shape: {X_regression.shape}, y_regression shape: {y_regression.shape}")
else:
    print("Erro: 'mental_wellness_index_0_100' não encontrado no df_model. Verifique o pipeline de preparação de dados.")

print(X_regression.head())
print(y_regression.head())

# 1. Inicializar um MLPRegressor
# Parâmetros iniciais razoáveis, similar ao MLPClassifier anterior, mas para regressão.
# Usamos um max_iter um pouco maior para tentar garantir a convergência.
mlp_regressor = MLPRegressor(
    hidden_layer_sizes=(100,),
    max_iter=1000, # Aumentado para melhor chance de convergência
    random_state=42,
    solver='adam', # O 'adam' é geralmente uma boa escolha para datasets médios a grandes
    activation='relu' # 'relu' é comum para camadas ocultas
)

# 2. Definir uma estratégia de validação cruzada KFold
kf_reg = KFold(n_splits=10, shuffle=True, random_state=42)

# 3. Definir os scorers para as métricas de regressão
mae_scorer_reg = make_scorer(mean_absolute_error, greater_is_better=False)
rmse_scorer_reg = make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)
r2_scorer_reg = make_scorer(r2_score)

# 4. Calcular as pontuações de validação cruzada para cada métrica
mae_scores_mlp_reg = -cross_val_score(mlp_regressor, X_regression, y_regression, cv=kf_reg, scoring=mae_scorer_reg)
rmse_scores_mlp_reg = -cross_val_score(mlp_regressor, X_regression, y_regression, cv=kf_reg, scoring=rmse_scorer_reg)
r2_scores_mlp_reg = cross_val_score(mlp_regressor, X_regression, y_regression, cv=kf_reg, scoring=r2_scorer_reg)

# 5. Imprimir a média e o desvio padrão de cada métrica
print("\nDesempenho do Multi-layer Perceptron (MLP) para Regressão (Cross-Validation):")
print(f"MAE  \u2192 mean={mae_scores_mlp_reg.mean():.4f}, std={mae_scores_mlp_reg.std():.4f}")
print(f"RMSE \u2192 mean={rmse_scores_mlp_reg.mean():.4f}, std={rmse_scores_mlp_reg.std():.4f}")
print(f"R\u00b2   \u2192 mean={r2_scores_mlp_reg.mean():.4f}, std={r2_scores_mlp_reg.std():.4f}")

"""Secção de classificação: criação de classes de bem‑estar mental."""

df_classification = num_df.copy()

# Criar classes a partir de 'mental_wellness_index_0_100'
# Definir os limites para as classes (ajustar conforme necessário)
# Por exemplo, 33º e 66º percentis para criar 3 classes
lower_bound = df_classification['mental_wellness_index_0_100'].quantile(0.33)
upper_bound = df_classification['mental_wellness_index_0_100'].quantile(0.66)

def categorize_wellness(score):
    if score <= lower_bound:
        return 'Baixo'
    elif score <= upper_bound:
        return 'Médio'
    else:
        return 'Alto'

df_classification['mental_wellness_class'] = df_classification['mental_wellness_index_0_100'].apply(categorize_wellness)

# Remover a coluna numérica original E qualquer outra coluna que possa causar vazamento de dados
# mental_wellness_index_0_100_class está causando vazamento, então também a removeremos
columns_to_drop = ['mental_wellness_index_0_100', 'mental_wellness_index_0_100_class']
df_classification = df_classification.drop(columns=columns_to_drop, errors='ignore')

print('Distribuição das classes de bem-estar mental:')
print(df_classification['mental_wellness_class'].value_counts())

df_classification.head()

"""Task: ajustar hiperparâmetros para a DecisionTreeClassifier (GridSearchCV) e preparar dados de classificação (X, y)."""

from sklearn.preprocessing import LabelEncoder

# 1. Identificar a coluna alvo
target_column_classification = 'mental_wellness_class'

# 2. Criar uma cópia do DataFrame para evitar modificações indesejadas
df_prepared_classification = df_classification.copy()

# 3. Utilizar LabelEncoder para transformar a coluna alvo
le = LabelEncoder()
df_prepared_classification[target_column_classification] = le.fit_transform(df_prepared_classification[target_column_classification])

# 4. Armazenar os nomes das classes originais para referência futura
classification_class_names = le.classes_

# 5. Separar o DataFrame em features (X) e alvo (y)
X_classification = df_prepared_classification.drop(columns=[target_column_classification])
y_classification = df_prepared_classification[target_column_classification]

print(f"Target column '{target_column_classification}' encoded successfully.")
print(f"Original class names: {classification_class_names}")
print(f"Shape of X for classification: {X_classification.shape}")
print(f"Shape of y for classification: {y_classification.shape}")
print(X_classification.head())
print(y_classification.head())

"""Notas: otimização da DecisionTreeClassifier e ajuste do uso de palette no seaborn."""

from sklearn.model_selection import GridSearchCV, KFold
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import make_scorer, f1_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# 1. Definir o modelo base
dt_classifier = DecisionTreeClassifier(random_state=42)

# 2. Definir o grid de hiperparâmetros para otimização
param_grid = {
    'max_depth': [3, 5, 7, 10, None], # None means no limit
    'min_samples_leaf': [1, 5, 10, 20],
    'criterion': ['gini', 'entropy']
}

# 3. Definir a validação cruzada (KFold)
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# 4. Definir o scorer para F1-Score (macro)
f1_macro_scorer = make_scorer(f1_score, average='macro')

# 5. Inicializar GridSearchCV
grid_search = GridSearchCV(
    estimator=dt_classifier,
    param_grid=param_grid,
    cv=kf,
    scoring=f1_macro_scorer,
    n_jobs=-1, # Usar todos os cores disponíveis
    verbose=1
)

# 6. Executar a busca pelos melhores hiperparâmetros
grid_search.fit(X_classification, y_classification)

print(f"Melhores hiperparâmetros: {grid_search.best_params_}")
print(f"Melhor F1-Score (macro) da validação cruzada: {grid_search.best_score_:.4f}")

# 7. Obter o melhor modelo
best_classifier = grid_search.best_estimator_

# 8. Avaliar o melhor modelo usando as métricas de cross-validation (após otimização)
accuracy_scores = cross_val_score(best_classifier, X_classification, y_classification, cv=kf, scoring='accuracy')
precision_macro_scores = cross_val_score(best_classifier, X_classification, y_classification, cv=kf, scoring='precision_macro')
recall_macro_scores = cross_val_score(best_classifier, X_classification, y_classification, cv=kf, scoring='recall_macro')
f1_macro_scores_eval = cross_val_score(best_classifier, X_classification, y_classification, cv=kf, scoring=f1_macro_scorer)

print(f"\nDesempenho do Melhor Modelo (Cross-Validation):")
print(f"Accuracy  → mean={accuracy_scores.mean():.4f}, std={accuracy_scores.std():.4f}")
print(f"Precision (macro) → mean={precision_macro_scores.mean():.4f}, std={precision_macro_scores.std():.4f}")
print(f"Recall (macro)    → mean={recall_macro_scores.mean():.4f}, std={recall_macro_scores.std():.4f}")
print(f"F1-Score (macro)  → mean={f1_macro_scores_eval.mean():.4f}, std={f1_macro_scores_eval.std():.4f}")

# 9. Exibir as importâncias das features do melhor modelo
importances = pd.Series(best_classifier.feature_importances_, index=X_classification.columns)
importances = importances.sort_values(ascending=False).head(12)

plt.figure(figsize=(10,6))
sns.barplot(x=importances.values, y=importances.index, hue=importances.index, palette='viridis', legend=False)
plt.title(f"Feature Importances — Melhor DecisionTreeClassifier")
plt.xlabel("Importância")
plt.ylabel("Variável")
plt.tight_layout()
plt.show()

# 10. Exibir a estrutura da árvore de decisão do melhor modelo
plt.figure(figsize=(20,12))
plot_tree(best_classifier, feature_names=X_classification.columns, class_names=[str(c) for c in classification_class_names], filled=True, fontsize=8)
plt.title(f"Decision Tree Structure — Melhor DecisionTreeClassifier")
plt.show()

"""Secção: treino e avaliação de RandomForestClassifier (classificação) com cross‑validation."""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

def run_random_forest_classifier_cv(X, y, class_names, title="RandomForestClassifier CV", k=10):

    # Model
    model = RandomForestClassifier(
        random_state=42,
        n_estimators=100, # Default number of trees
        max_depth=None,   # No limit on depth
        min_samples_leaf=1, # Minimum number of samples required to be at a leaf node
        class_weight='balanced' # To handle potential class imbalance
    )

    # Define split for cross-validation
    kf = KFold(n_splits=k, shuffle=True, random_state=42)

    # Define scorers for classification metrics
    accuracy_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')
    precision_macro_scores = cross_val_score(model, X, y, cv=kf, scoring='precision_macro')
    recall_macro_scores = cross_val_score(model, X, y, cv=kf, scoring='recall_macro')
    f1_macro_scorer = make_scorer(f1_score, average='macro')
    f1_macro_scores = cross_val_score(model, X, y, cv=kf, scoring=f1_macro_scorer)

    print(f"Accuracy  → mean={accuracy_scores.mean():.4f}, std={accuracy_scores.std():.4f}")
    print(f"Precision (macro) → mean={precision_macro_scores.mean():.4f}, std={precision_macro_scores.std():.4f}")
    print(f"Recall (macro)    → mean={recall_macro_scores.mean():.4f}, std={recall_macro_scores.std():.4f}")
    print(f"F1-Score (macro)  → mean={f1_macro_scores.mean():.4f}, std={f1_macro_scores.std():.4f}")

    # Fit the model on the entire dataset to get feature importances
    model.fit(X, y)

    # IMPORTANCE DAS FEATURES
    importances = pd.Series(model.feature_importances_, index=X.columns)
    importances = importances.sort_values(ascending=False).head(12)

    plt.figure(figsize=(10,6))
    sns.barplot(x=importances.values, y=importances.index, hue=importances.index, palette='viridis', legend=False)
    plt.title(f"Feature Importances — {title}")
    plt.xlabel("Importância")
    plt.tight_layout()
    plt.show()

    return {
        "accuracy_mean": accuracy_scores.mean(),
        "precision_macro_mean": precision_macro_scores.mean(),
        "recall_macro_mean": recall_macro_scores.mean(),
        "f1_macro_mean": f1_macro_scores.mean(),
        "model": model,
        "importances": importances
    }

"""Chamada da função de avaliação do RandomForestClassifier categorizado."""

run_random_forest_classifier_cv(X_classification, y_classification, classification_class_names, title='RandomForestClassifier - Categorizado')

"""Resumo intermédio: RandomForestClassifier supera DecisionTreeClassifier em F1‑Score (macro) e utiliza melhor as features engenheiradas."""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, KFold
from sklearn.metrics import make_scorer, f1_score

# 1. Importações (já feitas no boilerplate, mas para clareza)
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.model_selection import GridSearchCV, KFold
# from sklearn.metrics import make_scorer, f1_score

# Certificar-se de que X_classification e y_classification estão disponíveis
# Assumimos que foram criados nos passos anteriores.
# Se não estiverem, precisaríamos recriá-los:
# X_classification = df_prepared_classification.drop(columns=[target_column_classification])
# y_classification = df_prepared_classification[target_column_classification]

# 2. Inicializar um RandomForestClassifier como estimador base
rf_classifier = RandomForestClassifier(random_state=42)

# 3. Definir o dicionário param_grid com os hiperparâmetros a serem otimizados
param_grid_rf = {
    'n_estimators': [50, 100, 200], # Número de árvores na floresta
    'max_depth': [None, 10, 20, 30], # Profundidade máxima da árvore
    'min_samples_leaf': [1, 2, 4], # Mínimo de amostras necessárias em um nó folha
    'class_weight': ['balanced', None] # Lidar com desbalanceamento de classes
}

# 4. Criar um objeto KFold para a validação cruzada
kf_rf = KFold(n_splits=10, shuffle=True, random_state=42)

# 5. Criar um scorer para otimizar o F1-Score (macro)
f1_macro_scorer_rf = make_scorer(f1_score, average='macro')

# 6. Instanciar GridSearchCV
grid_search_rf = GridSearchCV(
    estimator=rf_classifier,
    param_grid=param_grid_rf,
    cv=kf_rf,
    scoring=f1_macro_scorer_rf,
    n_jobs=-1, # Usar todos os núcleos disponíveis
    verbose=1
)

# 7. Ajustar o GridSearchCV aos dados de classificação
grid_search_rf.fit(X_classification, y_classification)

# 8. Imprimir os melhores hiperparâmetros encontrados
print(f"Melhores hiperparâmetros para RandomForestClassifier: {grid_search_rf.best_params_}")

# 9. Imprimir o melhor F1-Score (macro) obtido
print(f"Melhor F1-Score (macro) da validação cruzada para RandomForestClassifier: {grid_search_rf.best_score_:.4f}")

"""Secção: avaliação do RandomForestClassifier otimizado (métricas e importâncias de features)."""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import f1_score, make_scorer

# 1. Obter o melhor modelo
best_rf_classifier = grid_search_rf.best_estimator_

# 2. Definir a validação cruzada (KFold)
kf_eval = KFold(n_splits=10, shuffle=True, random_state=42)

# 3. Definir os scorers para as métricas de classificação
f1_macro_scorer_eval = make_scorer(f1_score, average='macro')

# 4. Calcular as métricas de desempenho usando cross_val_score
accuracy_scores_rf = cross_val_score(best_rf_classifier, X_classification, y_classification, cv=kf_eval, scoring='accuracy')
precision_macro_scores_rf = cross_val_score(best_rf_classifier, X_classification, y_classification, cv=kf_eval, scoring='precision_macro')
recall_macro_scores_rf = cross_val_score(best_rf_classifier, X_classification, y_classification, cv=kf_eval, scoring='recall_macro')
f1_macro_scores_rf = cross_val_score(best_rf_classifier, X_classification, y_classification, cv=kf_eval, scoring=f1_macro_scorer_eval)

# 5. Imprimir as médias e desvios padrão de cada métrica
print(f"\nDesempenho do Melhor RandomForestClassifier (Cross-Validation):")
print(f"Accuracy  \u2192 mean={accuracy_scores_rf.mean():.4f}, std={accuracy_scores_rf.std():.4f}")
print(f"Precision (macro) \u2192 mean={precision_macro_scores_rf.mean():.4f}, std={precision_macro_scores_rf.std():.4f}")
print(f"Recall (macro)    \u2192 mean={recall_macro_scores_rf.mean():.4f}, std={recall_macro_scores_rf.std():.4f}")
print(f"F1-Score (macro)  \u2192 mean={f1_macro_scores_rf.mean():.4f}, std={f1_macro_scores_rf.std():.4f}")

# 6. Exibir as importâncias das features do melhor modelo
# Fit the best model on the entire dataset to get feature importances
best_rf_classifier.fit(X_classification, y_classification)
importances_rf = pd.Series(best_rf_classifier.feature_importances_, index=X_classification.columns)
importances_rf = importances_rf.sort_values(ascending=False).head(12)

plt.figure(figsize=(10,6))
sns.barplot(x=importances_rf.values, y=importances_rf.index, hue=importances_rf.index, palette='viridis', legend=False)
plt.title(f"Feature Importances \u2014 Melhor RandomForestClassifier")
plt.xlabel("Import\u00e2ncia")
plt.ylabel("Vari\u00e1vel")
plt.tight_layout()
plt.show()

"""Notas: análise qualitativa das importâncias de features do RandomForestClassifier otimizado e implicações para seleção de features."""

from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score
import numpy as np

# 1. Inicializar um modelo GaussianNB
gnb_classifier = GaussianNB()

# 2. Definir uma estratégia de validação cruzada KFold
kf_gnb = KFold(n_splits=10, shuffle=True, random_state=42)

# 3. Definir os scorers para as métricas
f1_macro_scorer_gnb = make_scorer(f1_score, average='macro')
precision_macro_scorer_gnb = make_scorer(precision_score, average='macro', zero_division=0)
recall_macro_scorer_gnb = make_scorer(recall_score, average='macro', zero_division=0)

# 4. Calcular as pontuações de validação cruzada para cada métrica
accuracy_scores_gnb = cross_val_score(gnb_classifier, X_classification, y_classification, cv=kf_gnb, scoring='accuracy')
precision_scores_gnb = cross_val_score(gnb_classifier, X_classification, y_classification, cv=kf_gnb, scoring=precision_macro_scorer_gnb)
recall_scores_gnb = cross_val_score(gnb_classifier, X_classification, y_classification, cv=kf_gnb, scoring=recall_macro_scorer_gnb)
f1_scores_gnb = cross_val_score(gnb_classifier, X_classification, y_classification, cv=kf_gnb, scoring=f1_macro_scorer_gnb)

# 5. Imprimir a média e o desvio padrão de cada métrica
print("\nDesempenho do Gaussian Naive Bayes (Cross-Validation):")
print(f"Accuracy  \u2192 mean={accuracy_scores_gnb.mean():.4f}, std={accuracy_scores_gnb.std():.4f}")
print(f"Precision (macro) \u2192 mean={precision_scores_gnb.mean():.4f}, std={precision_scores_gnb.std():.4f}")
print(f"Recall (macro)    \u2192 mean={recall_scores_gnb.mean():.4f}, std={recall_scores_gnb.std():.4f}")
print(f"F1-Score (macro)  \u2192 mean={f1_scores_gnb.mean():.4f}, std={f1_scores_gnb.std():.4f}")

"""## Treinar e Avaliar o Classificador Support Vector Machine (SVM)

### Subtask:
Treinar um classificador Support Vector Machine (SVM) utilizando validação cruzada nos dados X_classification e y_classification. Avaliar o seu desempenho com as métricas de acurácia, precisão (macro), recall (macro) e F1-Score (macro).

**Reasoning**:
The subtask requires training and evaluating an SVM classifier using cross-validation and specific metrics. I will import the necessary libraries, initialize the SVC model, configure KFold for cross-validation, create appropriate scorers, and then compute and print the cross-validated performance metrics.
"""

from sklearn.svm import SVC
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score
import numpy as np

# 1. Inicializar um classificador SVC
svc_classifier = SVC(random_state=42)

# 2. Definir uma estratégia de validação cruzada KFold
kf_svc = KFold(n_splits=10, shuffle=True, random_state=42)

# 3. Definir os scorers para as métricas
f1_macro_scorer_svc = make_scorer(f1_score, average='macro', zero_division=0)
precision_macro_scorer_svc = make_scorer(precision_score, average='macro', zero_division=0)
recall_macro_scorer_svc = make_scorer(recall_score, average='macro', zero_division=0)

# 4. Calcular as pontuações de validação cruzada para cada métrica
accuracy_scores_svc = cross_val_score(svc_classifier, X_classification, y_classification, cv=kf_svc, scoring='accuracy')
precision_scores_svc = cross_val_score(svc_classifier, X_classification, y_classification, cv=kf_svc, scoring=precision_macro_scorer_svc)
recall_scores_svc = cross_val_score(svc_classifier, X_classification, y_classification, cv=kf_svc, scoring=recall_macro_scorer_svc)
f1_scores_svc = cross_val_score(svc_classifier, X_classification, y_classification, cv=kf_svc, scoring=f1_macro_scorer_svc)

# 5. Imprimir a média e o desvio padrão de cada métrica
print("\nDesempenho do Support Vector Machine (Cross-Validation):")
print(f"Accuracy  \u2192 mean={accuracy_scores_svc.mean():.4f}, std={accuracy_scores_svc.std():.4f}")
print(f"Precision (macro) \u2192 mean={precision_scores_svc.mean():.4f}, std={precision_scores_svc.std():.4f}")
print(f"Recall (macro)    \u2192 mean={recall_scores_svc.mean():.4f}, std={recall_scores_svc.std():.4f}")
print(f"F1-Score (macro)  \u2192 mean={f1_scores_svc.mean():.4f}, std={f1_scores_svc.std():.4f}")

"""## Treinar e Avaliar o Classificador Multi-layer Perceptron (MLP) (Deep Learning)

### Subtask:
Treinar um classificador Multi-layer Perceptron (MLP) como um modelo simples de Deep Learning utilizando validação cruzada nos dados X_classification e y_classification. Avaliar o seu desempenho com as métricas de acurácia, precisão (macro), recall (macro) e F1-Score (macro).

**Reasoning**:
The subtask requires training and evaluating an MLP classifier using cross-validation and specific metrics. I will import the necessary libraries, initialize the MLPClassifier model with reasonable parameters, configure KFold for cross-validation, create appropriate scorers, and then compute and print the cross-validated performance metrics.

**Reasoning**:
The `MLPClassifier` is still issuing `ConvergenceWarning`s even after increasing `max_iter` to 1000. To further address this, I will increase the `max_iter` parameter again to 2000, allowing more iterations for the model to converge.
"""

from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score
import numpy as np

# 1. Inicializar um MLPClassifier com um número maior de iterações
mlp_classifier = MLPClassifier(hidden_layer_sizes=(100,), max_iter=2000, random_state=42) # Aumentado max_iter para 2000

# 2. Definir uma estratégia de validação cruzada KFold
kf_mlp = KFold(n_splits=10, shuffle=True, random_state=42)

# 3. Definir os scorers para as métricas
f1_macro_scorer_mlp = make_scorer(f1_score, average='macro', zero_division=0)
precision_macro_scorer_mlp = make_scorer(precision_score, average='macro', zero_division=0)
recall_macro_scorer_mlp = make_scorer(recall_score, average='macro', zero_division=0)

# 4. Calcular as pontuações de validação cruzada para cada métrica
accuracy_scores_mlp = cross_val_score(mlp_classifier, X_classification, y_classification, cv=kf_mlp, scoring='accuracy')
precision_scores_mlp = cross_val_score(mlp_classifier, X_classification, y_classification, cv=kf_mlp, scoring=precision_macro_scorer_mlp)
recall_scores_mlp = cross_val_score(mlp_classifier, X_classification, y_classification, cv=kf_mlp, scoring=recall_macro_scorer_mlp)
f1_scores_mlp = cross_val_score(mlp_classifier, X_classification, y_classification, cv=kf_mlp, scoring=f1_macro_scorer_mlp)

# 5. Imprimir a média e o desvio padrão de cada métrica
print("\nDesempenho do Multi-layer Perceptron (MLP) (Cross-Validation):")
print(f"Accuracy  \u2192 mean={accuracy_scores_mlp.mean():.4f}, std={accuracy_scores_mlp.std():.4f}")
print(f"Precision (macro) \u2192 mean={precision_scores_mlp.mean():.4f}, std={precision_scores_mlp.std():.4f}")
print(f"Recall (macro)    \u2192 mean={recall_scores_mlp.mean():.4f}, std={recall_scores_mlp.std():.4f}")
print(f"F1-Score (macro)  \u2192 mean={f1_scores_mlp.mean():.4f}, std={f1_scores_mlp.std():.4f}")

"""## Comparar Todos os Modelos de Classificação

### Subtask:
Comparar as métricas de desempenho (Acurácia, Precisão, Recall, F1-Score) de todos os classificadores avaliados: Decision Tree otimizado, Random Forest inicial, Random Forest otimizado, Naive Bayes, SVM e MLP. Discutir os pontos fortes e fracos de cada um para o problema em questão.

**Reasoning**:
To compare all classification models, I will first gather the mean performance metrics (Accuracy, Precision, Recall, F1-Score) from the cross-validation results of each model and consolidate them into a pandas DataFrame. This will create a structured overview of each model's performance.
"""

import pandas as pd

# Gather the mean metrics for each model
metrics_data = {
    'Model': [
        'Decision Tree (Otimizado)',
        'Random Forest (Inicial)',
        'Random Forest (Otimizado)',
        'Gaussian Naive Bayes',
        'Support Vector Machine (SVM)',
        'Multi-layer Perceptron (MLP)'
    ],
    'Accuracy': [
        accuracy_scores.mean(),
        0.7634, # From previous output, `accuracy_mean` variable was not saved directly
        accuracy_scores_rf.mean(),
        accuracy_scores_gnb.mean(),
        accuracy_scores_svc.mean(),
        accuracy_scores_mlp.mean()
    ],
    'Precision (macro)': [
        precision_macro_scores.mean(),
        0.7568, # From previous output, `precision_macro_mean` variable was not saved directly
        precision_macro_scores_rf.mean(),
        precision_scores_gnb.mean(),
        precision_scores_svc.mean(),
        precision_scores_mlp.mean()
    ],
    'Recall (macro)': [
        recall_macro_scores.mean(),
        0.7585, # From previous output, `recall_macro_mean` variable was not saved directly
        recall_macro_scores_rf.mean(),
        recall_scores_gnb.mean(),
        recall_scores_svc.mean(),
        recall_scores_mlp.mean()
    ],
    'F1-Score (macro)': [
        f1_macro_scores_eval.mean(),
        0.7498, # From previous output, `f1_macro_mean` variable was not saved directly
        f1_macro_scores_rf.mean(),
        f1_scores_gnb.mean(),
        f1_scores_svc.mean(),
        f1_scores_mlp.mean()
    ]
}

# Create a DataFrame for comparison
df_comparison = pd.DataFrame(metrics_data)

# Format for better readability
df_comparison = df_comparison.round(4)

print("Tabela Comparativa de Métricas de Desempenho dos Classificadores:\n")
print(df_comparison)

"""## Comparação de Todos os Modelos de Classificação

Aqui, compilamos e comparamos as métricas de desempenho de todos os modelos de classificação avaliados no problema de prever as classes de bem-estar mental.

### Tabela Comparativa de Métricas de Desempenho (Médias de Cross-Validation):

| Métrica           | Decision Tree (Otimizado) | Random Forest (Inicial) | Random Forest (Otimizado) | Gaussian Naive Bayes | Support Vector Machine (SVM) | Multi-layer Perceptron (MLP) |
| :---------------- | :------------------------ | :---------------------- | :------------------------ | :------------------- | :--------------------------- | :--------------------------- |
| **Accuracy**      | 0.7354                    | 0.7634                  | **0.7891**                | 0.7069               | 0.7225                       | 0.7556                       |
| **Precision (macro)** | 0.7415                    | 0.7568                  | **0.7873**                | 0.7075               | 0.7305                       | 0.7489                       |
| **Recall (macro)**    | 0.7445                    | 0.7585                  | **0.7870**                | 0.7032               | 0.7295                       | 0.7517                       |
| **F1-Score (macro)**  | 0.7281                    | 0.7498                  | **0.7790**                | 0.6813               | 0.7196                       | 0.7463                       |

### Discussão Comparativa dos Modelos:

1.  **RandomForestClassifier (Otimizado)**:
    *   **Desempenho**: Claramente o **melhor modelo** em todas as métricas, atingindo os maiores valores de Accuracy, Precision, Recall e F1-Score. A otimização de hiperparâmetros foi crucial, melhorando o desempenho em relação à versão inicial do Random Forest.
    *   **Pontos Fortes**: A natureza de ensemble do Random Forest (combinando múltiplas árvores) reduz o overfitting e melhora a capacidade de generalização. Ele também se mostrou eficaz na utilização das features construídas (`screen_stress_index`, `sleep_score`), indicando sua capacidade de capturar relações complexas.
    *   **Pontos Fracos**: Mais complexo e computacionalmente mais caro que uma única árvore, mas o custo é justificado pelo desempenho superior.

2.  **RandomForestClassifier (Inicial)**:
    *   **Desempenho**: Superou o Decision Tree otimizado e os modelos base (Naive Bayes, SVM, MLP). Isso demonstra a robustez do Random Forest mesmo com parâmetros padrão, evidenciando o poder dos métodos de ensemble.
    *   **Pontos Fortes**: Bom desempenho inicial, robustez contra overfitting.
    *   **Pontos Fracos**: Pode ser melhorado com otimização, como demonstrado pela versão otimizada.

3.  **Multi-layer Perceptron (MLP)**:
    *   **Desempenho**: Apresentou um desempenho respeitável, sendo o segundo melhor modelo em termos de F1-Score (macro), ficando logo abaixo do Random Forest otimizado. Mesmo sem otimização de hiperparâmetros, demonstrou boa capacidade de aprendizado.
    *   **Pontos Fortes**: Capacidade de capturar relações não lineares e complexas nos dados, que pode ser benéfica para problemas de bem-estar mental que são inerentemente não lineares.
    *   **Pontos Fracos**: Atingiu `ConvergenceWarning`s mesmo com `max_iter` aumentado, indicando que a otimização da rede neural pode ser sensível e exigir mais ajustes de hiperparâmetros (e.g., `learning_rate`, arquitetura da rede, função de ativação) para atingir a convergência ideal e um desempenho ainda melhor. Também é mais exigente computacionalmente e menos interpretável que modelos baseados em árvores.

4.  **DecisionTreeClassifier (Otimizado)**:
    *   **Desempenho**: Ficou em terceiro lugar, com desempenho moderado. A otimização de hiperparâmetros ajudou a controlá-lo, mas ainda não alcançou a performance dos modelos de ensemble.
    *   **Pontos Fortes**: Simplicidade e interpretabilidade (fácil de entender a lógica de decisão), rápido para treinar.
    *   **Pontos Fracos**: Mais propenso a overfitting (mesmo otimizado, seu `max_depth` limitado a 3 pode ter restringido sua capacidade de aprender, enquanto uma profundidade maior poderia levar a overfitting). Não utilizou efetivamente todas as features construídas (algumas tinham importância zero).

5.  **Support Vector Machine (SVM)**:
    *   **Desempenho**: Apresentou um desempenho abaixo dos modelos baseados em árvores e MLP. Seu F1-Score de 0.7196 sugere que, com os parâmetros padrão, pode não ter capturado bem as complexidades do problema.
    *   **Pontos Fortes**: Eficaz em espaços de alta dimensão, robusto contra overfitting em alguns cenários e pode funcionar bem com limites de decisão não lineares usando kernels.
    *   **Pontos Fracos**: Sensível à escala das features (o que foi tratado com `StandardScaler` neste pipeline), e pode ser computacionalmente caro com grandes datasets. A escolha do kernel e a otimização de hiperparâmetros (`C`, `gamma`) são cruciais e não foram exploradas neste passo.

6.  **Gaussian Naive Bayes**:
    *   **Desempenho**: O modelo com o pior desempenho entre todos os avaliados, com um F1-Score (macro) de apenas 0.6813.
    *   **Pontos Fortes**: Simples, rápido para treinar e fazer previsões. Funciona bem em datasets com muitas features e é eficaz com dados categóricos ou onde a suposição de independência das features se sustenta.
    *   **Pontos Fracos**: A suposição de independência condicional das features (dada a classe) é forte e raramente verdadeira em datasets reais, o que provavelmente contribuiu para seu baixo desempenho neste caso. Também não pode capturar interações entre features.

### Conclusão:

O **RandomForestClassifier Otimizado** é o modelo mais indicado para este problema de classificação do bem-estar mental, superando consistentemente os demais em todas as métricas de desempenho. O MLP também demonstrou um bom potencial. Modelos como Naive Bayes e SVM, sem otimização de hiperparâmetros mais profunda, não se mostraram tão eficazes para este dataset e problema específico.

## Sumarizar os resultados e sugerir próximos passos

### Subtask:
Sumarizar os resultados da otimização e comparação dos modelos de classificação, destacando o modelo de melhor desempenho e os próximos passos sugeridos.

## Sumarizar os resultados e sugerir próximos passos

### Subtask:
Sumarizar os resultados da otimização e comparação dos modelos de classificação, destacando o modelo de melhor desempenho e os próximos passos sugeridos.

---

## Análise e Comparação de Todos os Modelos de Classificação

Nesta seção, apresentamos uma análise abrangente dos resultados de todos os classificadores avaliados: Decision Tree (otimizado), Random Forest (inicial), Random Forest (otimizado), Gaussian Naive Bayes, Support Vector Machine (SVM) e Multi-layer Perceptron (MLP).

### 1. Tabela Comparativa de Métricas de Desempenho (Médias de Cross-Validation):

| Model | Accuracy | Precision (macro) | Recall (macro) | F1-Score (macro) |
| :--------------------------- | :------- | :---------------- | :------------- | :--------------- |
| Decision Tree (Otimizado)    | 0.7354   | 0.7415            | 0.7445         | 0.7281           |
| Random Forest (Inicial)      | 0.7634   | 0.7568            | 0.7585         | 0.7498           |
| **Random Forest (Otimizado)**| **0.7891** | **0.7873**        | **0.7870**     | **0.7790**       |
| Gaussian Naive Bayes         | 0.7069   | 0.7075            | 0.7032         | 0.6813           |
| Support Vector Machine (SVM) | 0.7225   | 0.7305            | 0.7295         | 0.7196           |
| Multi-layer Perceptron (MLP) | 0.7556   | 0.7489            | 0.7517         | 0.7463           |

### 2. Modelo de Melhor Desempenho

O **RandomForestClassifier Otimizado** demonstrou o melhor desempenho geral, superando todos os outros modelos em todas as métricas de avaliação (Accuracy, Precision, Recall e F1-Score). Com um F1-Score (macro) de 0.7790, ele se destaca como o modelo mais eficaz para classificar as classes de bem-estar mental neste dataset.

### 3. Análise da Importância das Features

*   **Decision Tree (Otimizado)**: Concentrou a importância das features majoritariamente em `productivity_0_100` (0.5629) e `stress_level_0_10` (0.3178), com outras features tendo importância zero. Isso se deve à sua profundidade limitada e natureza de árvore única, que pode não capturar interações complexas.
*   **Random Forest (Inicial e Otimizado)**: Ambos os modelos Random Forest distribuíram a importância de forma mais equilibrada por um conjunto maior de features. Destacam-se `productivity_0_100`, `screen_stress_index` (feature construída), `stress_level_0_10` e `sleep_score` (feature construída). A otimização aumentou a importância de `screen_stress_index`, mostrando que o modelo aprimorado conseguiu explorar melhor essa interação. A capacidade do Random Forest de considerar múltiplas árvores e interações de features complexas o torna mais robusto e eficaz na identificação de múltiplos fatores influentes.

### 4. Por que o Random Forest Otimizado foi Superior

O **RandomForestClassifier Otimizado** foi superior devido a uma combinação de fatores:

*   **Natureza Ensemble**: Como um método de ensemble, o Random Forest combina as previsões de múltiplas árvores de decisão, o que geralmente reduz o overfitting e melhora a generalização em comparação com uma única árvore (Decision Tree).
*   **Otimização de Hiperparâmetros**: A busca por hiperparâmetros ideais via `GridSearchCV` permitiu que o modelo Random Forest atingisse seu potencial máximo para este dataset, resultando em métricas de desempenho superiores em relação à sua versão com parâmetros padrão.
*   **Capacidade de Capturar Interações**: O Random Forest consegue modelar relações não lineares e interações complexas entre as features de forma mais eficaz, o que é crucial, como visto pela importância das features construídas (`screen_stress_index`, `sleep_score`).

### 5. Sugestões de Próximos Passos

Para aprimorar ainda mais este projeto, as seguintes ações são recomendadas:

1.  **Explorar Modelos de Boosting**: Considerar a implementação e otimização de modelos de Gradient Boosting Machines, como XGBoost ou LightGBM, que são conhecidos por seu alto desempenho e capacidade de lidar com relações complexas nos dados.
2.  **Otimização Adicional de Outros Modelos**: Realizar otimização de hiperparâmetros para o MLPClassifier. Embora o `max_iter` tenha sido aumentado, warnings de convergência ainda foram observados, indicando que uma busca por outros hiperparâmetros (como arquitetura da rede, taxa de aprendizado, regularização) pode ser benéfica.
3.  **Análise de Erros Detalhada**: Investigar os casos em que o Random Forest Otimizado comete erros (falsos positivos/negativos) para identificar padrões, compreender as limitações do modelo e talvez descobrir novas oportunidades para engenharia de features.
4.  **Refinamento da Engenharia de Features**: A alta importância das features construídas (`screen_stress_index`, `sleep_score`) valida essa abordagem. Explorar a criação de outras features de interação ou transformações não lineares baseadas nos insights do negócio pode revelar ainda mais padrões.
5.  **Análise de Balanceamento de Classes**: Embora o `class_weight='balanced'` tenha sido considerado, uma análise mais aprofundada do balanceamento das classes de bem-estar mental e a aplicação de técnicas mais específicas (como SMOTE ou ADASYN) podem ser consideradas se o desbalanceamento for um problema.
6.  **Interpretabilidade do Modelo**: Para além das importâncias das features, explorar ferramentas de interpretabilidade de modelos (como SHAP ou LIME) para entender melhor como as previsões são feitas, especialmente para o Random Forest, que é um modelo de caixa preta.

Esta abordagem iterativa, focando em otimização, análise crítica e exploração contínua, é fundamental para construir modelos preditivos ainda mais robustos e eficazes.

## Sumarizar os resultados e sugerir próximos passos

### Subtask:
Sumarizar os resultados da otimização e comparação dos modelos de classificação, destacando o modelo de melhor desempenho e os próximos passos sugeridos.

---

## Análise e Comparação de Todos os Modelos de Classificação

Nesta seção, apresentamos uma análise abrangente dos resultados de todos os classificadores avaliados: Decision Tree (otimizado), Random Forest (inicial), Random Forest (otimizado), Gaussian Naive Bayes, Support Vector Machine (SVM) e Multi-layer Perceptron (MLP).

### 1. Tabela Comparativa de Métricas de Desempenho (Médias de Cross-Validation):

| Model | Accuracy | Precision (macro) | Recall (macro) | F1-Score (macro) |
| :--------------------------- | :------- | :---------------- | :------------- | :--------------- |
| Decision Tree (Otimizado)    | 0.7354   | 0.7415            | 0.7445         | 0.7281           |
| Random Forest (Inicial)      | 0.7634   | 0.7568            | 0.7585         | 0.7498           |
| **Random Forest (Otimizado)**| **0.7891** | **0.7873**        | **0.7870**     | **0.7790**       |
| Gaussian Naive Bayes         | 0.7069   | 0.7075            | 0.7032         | 0.6813           |
| Support Vector Machine (SVM) | 0.7225   | 0.7305            | 0.7295         | 0.7196           |
| Multi-layer Perceptron (MLP) | 0.7556   | 0.7489            | 0.7517         | 0.7463           |

### 2. Modelo de Melhor Desempenho

O **RandomForestClassifier Otimizado** demonstrou o melhor desempenho geral, superando todos os outros modelos em todas as métricas de avaliação (Accuracy, Precision, Recall e F1-Score). Com um F1-Score (macro) de 0.7790, ele se destaca como o modelo mais eficaz para classificar as classes de bem-estar mental neste dataset.

### 3. Análise da Importância das Features

*   **Decision Tree (Otimizado)**: Concentrou a importância das features majoritariamente em `productivity_0_100` (0.5629) e `stress_level_0_10` (0.3178), com outras features tendo importância zero. Isso se deve à sua profundidade limitada e natureza de árvore única, que pode não capturar interações complexas.
*   **Random Forest (Inicial e Otimizado)**: Ambos os modelos Random Forest distribuíram a importância de forma mais equilibrada por um conjunto maior de features. Destacam-se `productivity_0_100`, `screen_stress_index` (feature construída), `stress_level_0_10` e `sleep_score` (feature construída). A otimização aumentou a importância de `screen_stress_index`, mostrando que o modelo aprimorado conseguiu explorar melhor essa interação. A capacidade do Random Forest de considerar múltiplas árvores e interações de features complexas o torna mais robusto e eficaz na identificação de múltiplos fatores influentes.

### 4. Por que o Random Forest Otimizado foi Superior

O **RandomForestClassifier Otimizado** foi superior devido a uma combinação de fatores:

*   **Natureza Ensemble**: Como um método de ensemble, o Random Forest combina as previsões de múltiplas árvores de decisão, o que geralmente reduz o overfitting e melhora a generalização em comparação com uma única árvore (Decision Tree).
*   **Otimização de Hiperparâmetros**: A busca por hiperparâmetros ideais via `GridSearchCV` permitiu que o modelo Random Forest atingisse seu potencial máximo para este dataset, resultando em métricas de desempenho superiores em relação à sua versão com parâmetros padrão.
*   **Capacidade de Capturar Interações**: O Random Forest consegue modelar relações não lineares e interações complexas entre as features de forma mais eficaz, o que é crucial, como visto pela importância das features construídas (`screen_stress_index`, `sleep_score`).

### 5. Sugestões de Próximos Passos

Para aprimorar ainda mais este projeto, as seguintes ações são recomendadas:

1.  **Explorar Modelos de Boosting**: Considerar a implementação e otimização de modelos de Gradient Boosting Machines, como XGBoost ou LightGBM, que são conhecidos por seu alto desempenho e capacidade de lidar com relações complexas nos dados.
2.  **Otimização Adicional de Outros Modelos**: Realizar otimização de hiperparâmetros para o MLPClassifier. Embora o `max_iter` tenha sido aumentado, warnings de convergência ainda foram observados, indicando que uma busca por outros hiperparâmetros (como arquitetura da rede, taxa de aprendizado, regularização) pode ser benéfica.
3.  **Análise de Erros Detalhada**: Investigar os casos em que o Random Forest Otimizado comete erros (falsos positivos/negativos) para identificar padrões, compreender as limitações do modelo e talvez descobrir novas oportunidades para engenharia de features.
4.  **Refinamento da Engenharia de Features**: A alta importância das features construídas (`screen_stress_index`, `sleep_score`) valida essa abordagem. Explorar a criação de outras features de interação ou transformações não lineares baseadas nos insights do negócio pode revelar ainda mais padrões.
5.  **Análise de Balanceamento de Classes**: Embora o `class_weight='balanced'` tenha sido considerado, uma análise mais aprofundada do balanceamento das classes de bem-estar mental e a aplicação de técnicas mais específicas (como SMOTE ou ADASYN) podem ser consideradas se o desbalanceamento for um problema.
6.  **Interpretabilidade do Modelo**: Para além das importâncias das features, explorar ferramentas de interpretabilidade de modelos (como SHAP ou LIME) para entender melhor como as previsões são feitas, especialmente para o Random Forest, que é um modelo de caixa preta.

Esta abordagem iterativa, focando em otimização, análise crítica e exploração contínua, é fundamental para construir modelos preditivos ainda mais robustos e eficazes.

## Perform K-Means Clustering and Evaluate Optimal Number of Clusters

### Subtask:
Perform K-Means clustering on the preprocessed features (`X_classification`) and evaluate the optimal number of clusters using silhouette score, Calinski-Harabasz score, and Davies-Bouldin score.

**Reasoning**:
The subtask requires performing K-Means clustering and evaluating the optimal number of clusters using silhouette score, Calinski-Harabasz score, and Davies-Bouldin score. I will import the necessary libraries, loop through a range of cluster numbers, fit KMeans, calculate the scores, and store them. Then, I will plot these scores.
"""

from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Initialize lists to store scores
silhouette_scores = []
calinski_harabasz_scores = []
davies_bouldin_scores = []

# Define a range for the number of clusters
k_range = range(2, 16) # Start from 2 as silhouette score requires at least 2 clusters

for k in k_range:
    # Initialize KMeans model
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10) # n_init='auto' or explicit number for robust results

    # Fit KMeans to the data and predict clusters
    cluster_labels = kmeans.fit_predict(X_classification)

    # Calculate evaluation scores
    silhouette_avg = silhouette_score(X_classification, cluster_labels)
    calinski_harabasz_avg = calinski_harabasz_score(X_classification, cluster_labels)
    davies_bouldin_avg = davies_bouldin_score(X_classification, cluster_labels)

    # Append scores to lists
    silhouette_scores.append(silhouette_avg)
    calinski_harabasz_scores.append(calinski_harabasz_avg)
    davies_bouldin_scores.append(davies_bouldin_avg)

# Plot the evaluation scores
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
plt.plot(k_range, silhouette_scores, marker='o')
plt.title('Silhouette Score')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Score')
plt.grid(True)

plt.subplot(1, 3, 2)
plt.plot(k_range, calinski_harabasz_scores, marker='o', color='green')
plt.title('Calinski-Harabasz Score')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Score')
plt.grid(True)

plt.subplot(1, 3, 3)
plt.plot(k_range, davies_bouldin_scores, marker='o', color='red')
plt.title('Davies-Bouldin Score')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Score')
plt.grid(True)

plt.tight_layout()
plt.show()

print("K-Means clustering and evaluation complete. Please review the plots to determine the optimal number of clusters.")

"""## Analisar as Métricas de Avaliação de Clusterização e Selecionar o K Ótimo

### Subtask:
Analisar os gráficos das métricas de avaliação (Silhouette, Calinski-Harabasz, Davies-Bouldin) gerados na etapa anterior para determinar o número ótimo de clusters (K).

## Análise das Métricas de Avaliação de Clusterização e Seleção do K Ótimo

Com base nos gráficos gerados na etapa anterior para as métricas Silhouette Score, Calinski-Harabasz Score e Davies-Bouldin Score, faremos a análise para determinar o número ótimo de clusters (K):

### 1. Análise do Silhouette Score:
*   **Interpretação**: O Silhouette Score mede quão similar um objeto é ao seu próprio cluster em comparação com outros clusters. Valores mais altos indicam melhor separação entre os clusters. O valor ideal é 1, e valores próximos de 0 indicam clusters sobrepostos ou objetos na fronteira entre clusters.
*   **Observação nos Gráficos**: No gráfico do Silhouette Score, observamos o maior pico em **K=2** (aproximadamente 0.49). Após K=2, o score decai e flutua, com uma queda notável em K=3 e K=4, e se mantendo em níveis mais baixos para K maiores. Há um pequeno repique em K=11 e K=14.

### 2. Análise do Calinski-Harabasz Score:
*   **Interpretação**: O Calinski-Harabasz Score (também conhecido como Variance Ratio Criterion) é a razão da dispersão entre clusters pela dispersão dentro dos clusters. Valores mais altos indicam clusters mais densos e bem separados. O valor ideal é o maior pico.
*   **Observação nos Gráficos**: O gráfico do Calinski-Harabasz Score mostra um pico proeminente em **K=3** (aproximadamente 615). Depois disso, o score geralmente diminui à medida que K aumenta, indicando que a separação entre os clusters não melhora significativamente, ou a coesão dentro deles diminui.

### 3. Análise do Davies-Bouldin Score:
*   **Interpretação**: O Davies-Bouldin Score mede a média de similaridade entre cada cluster e seu cluster mais similar. Valores mais baixos indicam melhor clusterização, com clusters mais compactos e bem separados. O valor ideal é 0, e buscamos o menor ponto (vale).
*   **Observação nos Gráficos**: O gráfico do Davies-Bouldin Score apresenta seu valor mais baixo em **K=2** (aproximadamente 0.72). O score aumenta após K=2, indicando uma piora na separação ou coesão dos clusters com K crescente. Há outro ponto relativamente baixo em K=3 e K=4, mas não tão baixo quanto em K=2.

### 4. Determinação do K Ótimo:

*   **Consenso**: Há um forte indício de **K=2** como o número ótimo de clusters, pois tanto o Silhouette Score quanto o Davies-Bouldin Score apontam para este valor como o melhor (maior Silhouette, menor Davies-Bouldin).
*   **Divergência**: O Calinski-Harabasz Score sugere **K=3** como o pico. No entanto, é comum que as métricas não apontem para um K único, e um compromisso ou uma análise mais aprofundada seja necessária.
*   **Considerações Finais**: Dada a consistência do Silhouette e Davies-Bouldin Scores, que medem aspectos complementares da qualidade dos clusters (separação/coesão e compacidade/separação), **K=2** parece ser o número mais robusto de clusters para este dataset. Um K=3 também pode ser uma opção viável, mas as evidências de K=2 são mais fortes.

**Conclusão**: Baseado na análise das três métricas de avaliação, o número ótimo de clusters para o dataset é **K=2**, com K=3 sendo uma alternativa a ser considerada.

## Treinar e Avaliar o Classificador XGBoost

### Subtask:
Treinar e avaliar um classificador XGBoost utilizando validação cruzada nos dados X_classification e y_classification. Avaliar o seu desempenho com as métricas de acurácia, precisão (macro), recall (macro) e F1-Score (macro).

**Reasoning**:
The subtask requires training and evaluating an XGBoost classifier using cross-validation and specific metrics. I will import the necessary libraries, initialize the XGBClassifier model, configure KFold for cross-validation, create appropriate scorers, and then compute and print the cross-validated performance metrics.
"""

from xgboost import XGBClassifier
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import make_scorer, f1_score, precision_score, recall_score
import numpy as np

# 1. Inicializar um XGBClassifier
# Usar eval_metric='logloss' ou 'mlogloss' para classificação e use_label_encoder=False
# para evitar warnings. 'mlogloss' é para multi-classe.
xgb_classifier = XGBClassifier(
    use_label_encoder=False,
    eval_metric='mlogloss',
    random_state=42,
    n_jobs=-1 # Usar todos os cores disponíveis
)

# 2. Definir uma estratégia de validação cruzada KFold
kf_xgb = KFold(n_splits=10, shuffle=True, random_state=42)

# 3. Definir os scorers para as métricas
f1_macro_scorer_xgb = make_scorer(f1_score, average='macro', zero_division=0)
precision_macro_scorer_xgb = make_scorer(precision_score, average='macro', zero_division=0)
recall_macro_scorer_xgb = make_scorer(recall_score, average='macro', zero_division=0)

# 4. Calcular as pontuações de validação cruzada para cada métrica
accuracy_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring='accuracy')
precision_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring=precision_macro_scorer_xgb)
recall_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring=recall_macro_scorer_xgb)
f1_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring=f1_macro_scorer_xgb)

# 5. Imprimir a média e o desvio padrão de cada métrica
print("\nDesempenho do XGBoost Classifier (Cross-Validation):")
print(f"Accuracy  \u2192 mean={accuracy_scores_xgb.mean():.4f}, std={accuracy_scores_xgb.std():.4f}")
print(f"Precision (macro) \u2192 mean={precision_scores_xgb.mean():.4f}, std={precision_scores_xgb.std():.4f}")
print(f"Recall (macro)    \u2192 mean={recall_scores_xgb.mean():.4f}, std={recall_scores_xgb.std():.4f}")
print(f"F1-Score (macro)  \u2192 mean={f1_scores_xgb.mean():.4f}, std={f1_scores_xgb.std():.4f}")

"""**Reasoning**:
The previous code block for XGBoost Classifier evaluation generated a `UserWarning` regarding the `use_label_encoder` parameter, indicating it is no longer used. To fix this, I will remove the `use_label_encoder=False` parameter from the `XGBClassifier` initialization.


"""

xgb_classifier = XGBClassifier(
    eval_metric='mlogloss',
    random_state=42,
    n_jobs=-1 # Usar todos os cores disponíveis
)

kf_xgb = KFold(n_splits=10, shuffle=True, random_state=42)

f1_macro_scorer_xgb = make_scorer(f1_score, average='macro', zero_division=0)
precision_macro_scorer_xgb = make_scorer(precision_score, average='macro', zero_division=0)
recall_macro_scorer_xgb = make_scorer(recall_score, average='macro', zero_division=0)

accuracy_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring='accuracy')
precision_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring=precision_macro_scorer_xgb)
recall_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring=recall_macro_scorer_xgb)
f1_scores_xgb = cross_val_score(xgb_classifier, X_classification, y_classification, cv=kf_xgb, scoring=f1_macro_scorer_xgb)

print("\nDesempenho do XGBoost Classifier (Cross-Validation):")
print(f"Accuracy  → mean={accuracy_scores_xgb.mean():.4f}, std={accuracy_scores_xgb.std():.4f}")
print(f"Precision (macro) → mean={precision_scores_xgb.mean():.4f}, std={precision_scores_xgb.std():.4f}")
print(f"Recall (macro)    → mean={recall_scores_xgb.mean():.4f}, std={recall_scores_xgb.std():.4f}")
print(f"F1-Score (macro)  → mean={f1_scores_xgb.mean():.4f}, std={f1_scores_xgb.std():.4f}")

"""## Explicar Desempenho e Sugerir Otimização

### Análise Comparativa Detalhada e Importância da Otimização de Hiperparâmetros

Nesta análise, aprofundamos os motivos pelos quais os modelos `Gaussian Naive Bayes`, `Support Vector Machine (SVM)`, `Multi-layer Perceptron (MLP)` e `XGBoost Classifier` apresentaram desempenho inferior ao `RandomForestClassifier` otimizado em nosso problema de classificação de bem-estar mental.

### 1. Desempenho dos Modelos sem Otimização Rigorosa:

A `Tabela Comparativa de Métricas de Desempenho` anterior demonstrou que o `RandomForestClassifier Otimizado` alcançou o melhor F1-Score (macro) de **0.7790**. Em contraste, os outros modelos obtiveram os seguintes F1-Scores (macro):

*   **Gaussian Naive Bayes**: 0.6813
*   **XGBoost Classifier**: 0.7186
*   **Support Vector Machine (SVM)**: 0.7196
*   **Multi-layer Perceptron (MLP)**: 0.7463

### 2. Razões para o Desempenho Inferior e a Necessidade de Otimização:

*   **Gaussian Naive Bayes:**
    *   **Motivo do Desempenho**: Este modelo é baseado na forte suposição de que as features são condicionalmente independentes, dada a classe. Na maioria dos datasets do mundo real, especialmente em dados de bem-estar mental onde vários fatores podem estar inter-relacionados (`stress_level` e `productivity`, por exemplo), essa suposição é frequentemente violada. A incapacidade do Naive Bayes de capturar interações complexas entre as features resultou em um desempenho notavelmente baixo.
    *   **Otimização**: Embora o Naive Bayes tenha menos hiperparâmetros para otimizar em comparação com outros modelos, a principal forma de "otimização" seria garantir que as suposições do modelo sejam adequadas aos dados ou pré-processar as features para tentar mitigar a dependência.

*   **Support Vector Machine (SVM):**
    *   **Motivo do Desempenho**: O SVM busca encontrar o hiperplano que melhor separa as classes. Seu desempenho é altamente dependente da escolha do kernel (linear, RBF, polinomial, etc.) e da otimização de seus hiperparâmetros chave, como `C` (parâmetro de regularização, que controla o trade-off entre classificação correta e margem máxima) e `gamma` (para kernels não lineares, que define o raio de influência das amostras de treino). No nosso caso, o SVM foi executado com parâmetros padrão, que podem não ser os mais adequados para a estrutura e complexidade do nosso dataset, resultando em um desempenho mediano.
    *   **Otimização**: Uma otimização de hiperparâmetros adequada para o SVM, usando técnicas como `GridSearchCV` ou `RandomizedSearchCV`, exploraria diferentes combinações de `C`, `gamma` (se usando RBF) e o tipo de kernel, o que tem o potencial de melhorar significativamente seu desempenho.

*   **Multi-layer Perceptron (MLP) (Deep Learning):**
    *   **Motivo do Desempenho**: O MLP é um modelo poderoso capaz de aprender relações não lineares complexas nos dados. No entanto, sua complexidade também o torna sensível a uma série de hiperparâmetros, incluindo a arquitetura da rede (número de camadas ocultas e neurônios por camada), a função de ativação, a taxa de aprendizado (`learning_rate`), o número máximo de iterações (`max_iter`) e o otimizador. Observamos `ConvergenceWarning`s mesmo após aumentar `max_iter`, indicando que o modelo pode não ter convergido para uma solução ótima, ou que outros hiperparâmetros estavam subótimos. Sem uma otimização direcionada, o MLP não conseguiu explorar seu potencial máximo.
    *   **Otimização**: A otimização de hiperparâmetros para o MLP envolveria a busca sistemática por uma arquitetura de rede ideal, ajuste da taxa de aprendizado e escolha do otimizador, o que poderia levar a melhorias substanciais no desempenho e resolver problemas de convergência.

*   **XGBoost Classifier:**
    *   **Motivo do Desempenho**: O XGBoost é um algoritmo de *gradient boosting* que constrói árvores de decisão sequencialmente, corrigindo os erros das árvores anteriores. Ele é conhecido por seu alto desempenho e flexibilidade. No entanto, como muitos modelos de ensemble e boosting, ele possui uma vasta gama de hiperparâmetros (`n_estimators`, `max_depth`, `learning_rate`, `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda`, etc.) que precisam ser ajustados para o dataset específico. A execução com parâmetros padrão, sem otimização, frequentemente não permite que o modelo atinja seu desempenho máximo, o que é o caso aqui, onde ele ficou atrás do Random Forest otimizado e até mesmo do MLP.
    *   **Otimização**: Uma otimização rigorosa dos hiperparâmetros do XGBoost é fundamental para liberar todo o seu poder preditivo. Ajustar parâmetros como a taxa de aprendizado, a profundidade máxima das árvores e os fatores de regularização pode levar a um modelo significativamente mais robusto e preciso.

### 3. A Importância da Otimização de Hiperparâmetros:

A otimização de hiperparâmetros é um passo crítico no pipeline de Machine Learning. Ela permite que os modelos sejam ajustados finamente aos padrões específicos e à estrutura de um dataset. Sem essa otimização, os modelos operam com configurações genéricas (padrão) que raramente são ideais. Um bom conjunto de hiperparâmetros pode:

*   **Melhorar a Generalização**: Reduzir o overfitting (o modelo aprende o ruído dos dados de treino) e o underfitting (o modelo não captura os padrões subjacentes), levando a um desempenho mais consistente em dados não vistos.
*   **Otimizar o Desempenho**: Maximizar métricas de avaliação específicas (como F1-Score, Accuracy, Precision, Recall) relevantes para o problema.
*   **Reduzir a Complexidade do Modelo**: Em alguns casos, uma otimização cuidadosa pode levar a modelos mais simples que ainda performam bem.

O exemplo do `RandomForestClassifier` (onde a otimização levou a uma melhoria notável em relação à versão inicial) ilustra perfeitamente este ponto.

### 4. Próximos Passos Sugeridos:

Para explorar o potencial máximo dos modelos `SVM`, `MLP` e `XGBoost Classifier`, o próximo passo crucial seria realizar uma **otimização de hiperparâmetros rigorosa** para cada um deles. Isso envolveria:

*   **Definição de Grids de Parâmetros**: Selecionar uma faixa razoável de valores para os hiperparâmetros mais importantes de cada modelo.
*   **Uso de `GridSearchCV` ou `RandomizedSearchCV`**: Aplicar essas técnicas de busca exaustiva ou aleatória em conjunto com a validação cruzada (`KFold`) para encontrar as melhores combinações de hiperparâmetros.
*   **Definição de Scorer Adequado**: Continuar usando o F1-Score (macro) como métrica principal para a otimização, dada a nossa necessidade de desempenho equilibrado entre as classes.

Ao seguir este processo, é provável que vejamos melhorias significativas no desempenho desses modelos, permitindo uma comparação mais justa e uma possível identificação de um modelo ainda mais robusto para a classificação do bem-estar mental.
"""

import pandas as pd

# Gather the mean metrics for each model
metrics_data = {
    'Model': [
        'Decision Tree (Otimizado)',
        'Random Forest (Inicial)',
        'Random Forest (Otimizado)',
        'Gaussian Naive Bayes',
        'Support Vector Machine (SVM)',
        'Multi-layer Perceptron (MLP)',
        'XGBoost Classifier'
    ],
    'Accuracy': [
        accuracy_scores.mean(),
        0.7634, # From previous output, `accuracy_mean` variable was not saved directly
        accuracy_scores_rf.mean(),
        accuracy_scores_gnb.mean(),
        accuracy_scores_svc.mean(),
        accuracy_scores_mlp.mean(),
        accuracy_scores_xgb.mean()
    ],
    'Precision (macro)': [
        precision_macro_scores.mean(),
        0.7568, # From previous output, `precision_macro_mean` variable was not saved directly
        precision_macro_scores_rf.mean(),
        precision_scores_gnb.mean(),
        precision_scores_svc.mean(),
        precision_scores_mlp.mean(),
        precision_scores_xgb.mean()
    ],
    'Recall (macro)': [
        recall_macro_scores.mean(),
        0.7585, # From previous output, `recall_macro_mean` variable was not saved directly
        recall_macro_scores_rf.mean(),
        recall_scores_gnb.mean(),
        recall_scores_svc.mean(),
        recall_scores_mlp.mean(),
        recall_scores_xgb.mean()
    ],
    'F1-Score (macro)': [
        f1_macro_scores_eval.mean(),
        0.7498, # From previous output, `f1_macro_mean` variable was not saved directly
        f1_macro_scores_rf.mean(),
        f1_scores_gnb.mean(),
        f1_scores_svc.mean(),
        f1_scores_mlp.mean(),
        f1_scores_xgb.mean()
    ]
}

# Create a DataFrame for comparison
df_comparison = pd.DataFrame(metrics_data)

# Format for better readability
df_comparison = df_comparison.round(4)

print("Tabela Comparativa de Métricas de Desempenho dos Classificadores:\n")
print(df_comparison)

"""## Comparação Final de Todos os Modelos de Classificação

Aqui, apresentamos uma análise abrangente das métricas de desempenho de todos os modelos de classificação avaliados para o problema de prever as classes de bem-estar mental. Esta seção consolida todas as discussões anteriores sobre desempenho.

### Tabela Comparativa de Métricas de Desempenho (Médias de Cross-Validation):

| Model | Accuracy | Precision (macro) | Recall (macro) | F1-Score (macro) |
| :--------------------------- | :------- | :---------------- | :------------- | :--------------- |
| Decision Tree (Otimizado)    | 0.7354   | 0.7415            | 0.7445         | 0.7281           |
| Random Forest (Inicial)      | 0.7634   | 0.7568            | 0.7585         | 0.7498           |
| **Random Forest (Otimizado)**| **0.7891** | **0.7873**        | **0.7870**     | **0.7790**       |
| Gaussian Naive Bayes         | 0.7069   | 0.7075            | 0.7032         | 0.6813           |
| Support Vector Machine (SVM) | 0.7225   | 0.7305            | 0.7295         | 0.7196           |
| Multi-layer Perceptron (MLP) | 0.7556   | 0.7489            | 0.7517         | 0.7463           |
| XGBoost Classifier           | 0.7275   | 0.7247            | 0.7296         | 0.7186           |

### Discussão Comparativa Detalhada dos Modelos:

1.  **RandomForestClassifier (Otimizado)**:
    *   **Desempenho**: Claramente o **melhor modelo** em todas as métricas, com os maiores valores de Accuracy, Precision, Recall e F1-Score (0.7790). A otimização de hiperparâmetros foi fundamental para aprimorar seu desempenho em relação à versão inicial.
    *   **Pontos Fortes**: A natureza de ensemble reduz o overfitting e melhora a generalização. Demonstrou eficácia na utilização de features construídas (`screen_stress_index`, `sleep_score`), indicando sua capacidade de capturar relações complexas e distribuir a importância das features de forma mais equilibrada.
    *   **Pontos Fracos**: Mais complexo e computacionalmente mais caro que uma única árvore, mas o custo é justificado pelo desempenho superior.

2.  **RandomForestClassifier (Inicial)**:
    *   **Desempenho**: Superou o Decision Tree otimizado e os modelos base (Naive Bayes, SVM, MLP, XGBoost sem otimização). Isso demonstra a robustez do Random Forest mesmo com parâmetros padrão.
    *   **Pontos Fortes**: Bom desempenho inicial, robustez contra overfitting.
    *   **Pontos Fracos**: Pode ser significativamente melhorado com otimização, como demonstrado pela versão otimizada.

3.  **Multi-layer Perceptron (MLP)**:
    *   **Desempenho**: Apresentou um desempenho respeitável (F1-Score de 0.7463), sendo o segundo melhor modelo. Mesmo com parâmetros padrão para a arquitetura, demonstrou boa capacidade de aprendizado.
    *   **Pontos Fortes**: Capacidade de capturar relações não lineares e complexas, essencial para problemas de bem-estar mental.
    *   **Pontos Fracos**: Atingiu `ConvergenceWarning`s mesmo com `max_iter` aumentado, indicando que uma otimização mais profunda de hiperparâmetros (arquitetura da rede, `learning_rate`, etc.) é necessária para alcançar o desempenho ideal. Mais exigente computacionalmente e menos interpretável que modelos baseados em árvores.

4.  **XGBoost Classifier:**
    *   **Desempenho**: Com parâmetros padrão, seu desempenho (F1-Score de 0.7186) ficou abaixo dos Random Forests e do MLP. Isso sugere que, como o Random Forest, ele se beneficiaria muito da otimização de hiperparâmetros.
    *   **Pontos Fortes**: É um algoritmo de *gradient boosting* robusto e geralmente de alto desempenho, conhecido por capturar relações complexas eficientemente.
    *   **Pontos Fracos**: O desempenho sem otimização não reflete seu potencial máximo. A interpretabilidade é mais complexa do que uma única árvore.

5.  **DecisionTreeClassifier (Otimizado)**:
    *   **Desempenho**: Apresentou desempenho moderado (F1-Score de 0.7281). A otimização de `max_depth` (para 3) ajudou a controlar o overfitting, mas limitou a capacidade da árvore de capturar relações mais complexas e interações de features.
    *   **Pontos Fortes**: Simplicidade, interpretabilidade e rapidez de treinamento.
    *   **Pontos Fracos**: Mais propenso a overfitting que ensembles e não utilizou efetivamente todas as features construídas (algumas tinham importância zero).

6.  **Support Vector Machine (SVM)**:
    *   **Desempenho**: Com parâmetros padrão, o SVM apresentou um F1-Score de 0.7196. Seu desempenho sugere que ele não capturou bem as complexidades do problema sem um ajuste fino.
    *   **Pontos Fortes**: Eficaz em espaços de alta dimensão e pode modelar limites de decisão não lineares com kernels apropriados.
    *   **Pontos Fracos**: Muito sensível à escolha do kernel e à otimização de hiperparâmetros como `C` e `gamma`, que não foram explorados a fundo neste passo. Pode ser computacionalmente caro para grandes datasets.

7.  **Gaussian Naive Bayes**:
    *   **Desempenho**: O modelo com o pior desempenho (F1-Score de 0.6813).
    *   **Pontos Fortes**: Simples, rápido para treinar e fazer previsões.
    *   **Pontos Fracos**: A forte suposição de independência condicional das features (dada a classe) é raramente verdadeira em datasets complexos como este, limitando severamente seu desempenho e incapacidade de capturar interações entre features.

### Conclusão:

O **RandomForestClassifier Otimizado** é o modelo mais indicado para este problema de classificação do bem-estar mental, superando consistentemente os demais em todas as métricas de desempenho. Sua natureza ensemble e a otimização de hiperparâmetros permitiram-lhe explorar efetivamente as features, incluindo as construídas. O MLP e o XGBoost também demonstram grande potencial, mas requerem otimização de hiperparâmetros para atingir seu melhor desempenho.

## Comparação Final de Todos os Modelos de Classificação

Aqui, apresentamos uma análise abrangente das métricas de desempenho de todos os modelos de classificação avaliados para o problema de prever as classes de bem-estar mental. Esta seção consolida todas as discussões anteriores sobre desempenho.

### Tabela Comparativa de Métricas de Desempenho (Médias de Cross-Validation):

| Model | Accuracy | Precision (macro) | Recall (macro) | F1-Score (macro) |
| :--------------------------- | :------- | :---------------- | :------------- | :--------------- |
| Decision Tree (Otimizado)    | 0.7354   | 0.7415            | 0.7445         | 0.7281           |
| Random Forest (Inicial)      | 0.7634   | 0.7568            | 0.7585         | 0.7498           |
| **Random Forest (Otimizado)**| **0.7891** | **0.7873**        | **0.7870**     | **0.7790**       |
| Gaussian Naive Bayes         | 0.7069   | 0.7075            | 0.7032         | 0.6813           |
| Support Vector Machine (SVM) | 0.7225   | 0.7305            | 0.7295         | 0.7196           |
| Multi-layer Perceptron (MLP) | 0.7556   | 0.7489            | 0.7517         | 0.7463           |
| XGBoost Classifier           | 0.7275   | 0.7247            | 0.7296         | 0.7186           |

### Discussão Comparativa Detalhada dos Modelos:

1.  **RandomForestClassifier (Otimizado)**:
    *   **Desempenho**: Claramente o **melhor modelo** em todas as métricas, com os maiores valores de Accuracy, Precision, Recall e F1-Score (0.7790). A otimização de hiperparâmetros foi fundamental para aprimorar seu desempenho em relação à versão inicial.
    *   **Pontos Fortes**: A natureza de ensemble reduz o overfitting e melhora a generalização. Demonstrou eficácia na utilização de features construídas (`screen_stress_index`, `sleep_score`), indicando sua capacidade de capturar relações complexas e distribuir a importância das features de forma mais equilibrada.
    *   **Pontos Fracos**: Mais complexo e computacionalmente mais caro que uma única árvore, mas o custo é justificado pelo desempenho superior.

2.  **RandomForestClassifier (Inicial)**:
    *   **Desempenho**: Superou o Decision Tree otimizado e os modelos base (Naive Bayes, SVM, MLP, XGBoost sem otimização). Isso demonstra a robustez do Random Forest mesmo com parâmetros padrão.
    *   **Pontos Fortes**: Bom desempenho inicial, robustez contra overfitting.
    *   **Pontos Fracos**: Pode ser significativamente melhorado com otimização, como demonstrado pela versão otimizada.

3.  **Multi-layer Perceptron (MLP)**:
    *   **Desempenho**: Apresentou um desempenho respeitável (F1-Score de 0.7463), sendo o segundo melhor modelo. Mesmo com parâmetros padrão para a arquitetura, demonstrou boa capacidade de aprendizado.
    *   **Pontos Fortes**: Capacidade de capturar relações não lineares e complexas, essencial para problemas de bem-estar mental.
    *   **Pontos Fracos**: Atingiu `ConvergenceWarning`s mesmo com `max_iter` aumentado, indicando que uma otimização mais profunda de hiperparâmetros (arquitetura da rede, `learning_rate`, etc.) é necessária para alcançar o desempenho ideal. Mais exigente computacionalmente e menos interpretável que modelos baseados em árvores.

4.  **XGBoost Classifier:**
    *   **Desempenho**: Com parâmetros padrão, seu desempenho (F1-Score de 0.7186) ficou abaixo dos Random Forests e do MLP. Isso sugere que, como o Random Forest, ele se beneficiaria muito da otimização de hiperparâmetros.
    *   **Pontos Fortes**: É um algoritmo de *gradient boosting* robusto e geralmente de alto desempenho, conhecido por capturar relações complexas eficientemente.
    *   **Pontos Fracos**: O desempenho sem otimização não reflete seu potencial máximo. A interpretabilidade é mais complexa do que uma única árvore.

5.  **DecisionTreeClassifier (Otimizado)**:
    *   **Desempenho**: Apresentou desempenho moderado (F1-Score de 0.7281). A otimização de `max_depth` (para 3) ajudou a controlar o overfitting, mas limitou a capacidade da árvore de capturar relações mais complexas e interações de features.
    *   **Pontos Fortes**: Simplicidade, interpretabilidade e rapidez de treinamento.
    *   **Pontos Fracos**: Mais propenso a overfitting que ensembles e não utilizou efetivamente todas as features construídas (algumas tinham importância zero).

6.  **Support Vector Machine (SVM)**:
    *   **Desempenho**: Com parâmetros padrão, o SVM apresentou um F1-Score de 0.7196. Seu desempenho sugere que ele não capturou bem as complexidades do problema sem um ajuste fino.
    *   **Pontos Fortes**: Eficaz em espaços de alta dimensão e pode modelar limites de decisão não lineares com kernels apropriados.
    *   **Pontos Fracos**: Muito sensível à escolha do kernel e à otimização de hiperparâmetros como `C` e `gamma`, que não foram explorados a fundo neste passo. Pode ser computacionalmente caro para grandes datasets.

7.  **Gaussian Naive Bayes**:
    *   **Desempenho**: O modelo com o pior desempenho (F1-Score de 0.6813).
    *   **Pontos Fortes**: Simples, rápido para treinar e fazer previsões.
    *   **Pontos Fracos**: A forte suposição de independência condicional das features (dada a classe) é raramente verdadeira em datasets complexos como este, limitando severamente seu desempenho e incapacidade de capturar interações entre features.

### Conclusão:

O **RandomForestClassifier Otimizado** é o modelo mais indicado para este problema de classificação do bem-estar mental, superando consistentemente os demais em todas as métricas de desempenho. Sua natureza ensemble e a otimização de hiperparâmetros permitiram-lhe explorar efetivamente as features, incluindo as construídas. O MLP e o XGBoost também demonstram grande potencial, mas requerem otimização de hiperparâmetros para atingir seu melhor desempenho.

## Sumário Final dos Modelos de Classificação

### 1. Modelo de Melhor Desempenho
O **RandomForestClassifier Otimizado** demonstrou o melhor desempenho geral, superando todos os outros modelos em todas as métricas de avaliação (Accuracy, Precision, Recall e F1-Score). Com um F1-Score (macro) de **0.7790**, ele se destaca como o modelo mais eficaz para classificar as classes de bem-estar mental neste dataset.

### 2. Principais Conclusões da Análise de Dados
*   **Superioridade do Random Forest Otimizado**: A otimização de hiperparâmetros para o `RandomForestClassifier` via `GridSearchCV` (com `class_weight`: `None`, `max_depth`: `None`, `min_samples_leaf`: `4`, `n_estimators`: `100`) foi crucial, resultando em desempenho superior com um F1-Score (macro) de **0.7790**.
*   **Impacto das Features Engenheiradas**: As features construídas, como `screen_stress_index` e `sleep_score`, foram identificadas como altamente importantes pelo Random Forest, indicando sua capacidade de capturar relações complexas que modelos mais simples ou menos otimizados podem ter ignorado.
*   **Necessidade de Otimização para Outros Modelos**: Modelos como `XGBoost Classifier`, `Support Vector Machine (SVM)` e `Multi-layer Perceptron (MLP)` apresentaram desempenho inferior (F1-Scores macro variando de 0.7186 a 0.7463) principalmente devido à execução com parâmetros padrão ou falta de otimização aprofundada. O `Gaussian Naive Bayes` teve o pior desempenho (F1-Score macro de 0.6813) devido às suas fortes suposições de independência.
*   **Classificação de Clusterização (K-Means)**: Com base nas métricas Silhouette e Davies-Bouldin, **K=2** foi identificado como o número ótimo de clusters, embora o Calinski-Harabasz tenha sugerido K=3.

### 3. Insights e Próximos Passos Sugeridos
*   **Otimização Profunda de Modelos com Potencial**: Para explorar o potencial total de modelos como `MLP` e `XGBoost`, é essencial realizar uma otimização rigorosa de hiperparâmetros (por exemplo, usando `GridSearchCV` ou `RandomizedSearchCV`) e considerar arquiteturas de rede mais sofisticadas para o MLP. Isso pode levar a ganhos de desempenho ainda maiores.
*   **Engenharia de Features Continuada**: A alta importância das features construídas sugere que a criação e o refinamento contínuos de novas features a partir das existentes podem ser uma estratégia valiosa para melhorar os modelos.
*   **Análise de Erros**: Uma análise detalhada dos casos de erro do `RandomForestClassifier Otimizado` pode revelar padrões e oportunidades para melhorias específicas no pré-processamento de dados ou no modelo.
*   **Modelos de Boosting Avançados**: A exploração de outros modelos de ensemble baseados em boosting, como LightGBM, pode ser benéfica, especialmente após a otimização de XGBoost para uma comparação mais justa.

Esta abordagem iterativa, focando em otimização, análise crítica e exploração contínua, é fundamental para construir modelos preditivos ainda mais robustos e eficazes para a classificação do bem-estar mental.

## Summary:

### Data Analysis Key Findings
*   The **Optimized RandomForestClassifier** emerged as the best-performing model for mental well-being classification, achieving an F1-Score (macro) of **0.7790**, along with the highest Accuracy, Precision, and Recall among all evaluated models.
*   **Hyperparameter optimization** was crucial for the Random Forest model, with parameters like `class_weight`: `None`, `max_depth`: `None`, `min_samples_leaf`: `4`, and `n_estimators`: `100` contributing significantly to its superior performance.
*   **Engineered features**, such as `screen_stress_index` and `sleep_score`, proved highly important to the Random Forest model, indicating their effectiveness in capturing complex relationships within the data.
*   Models like Multi-layer Perceptron (MLP) (F1-Score macro of 0.7463) and XGBoost Classifier (F1-Score macro of 0.7186) showed potential but performed less optimally due to being run with default parameters or lacking extensive optimization.
*   The Gaussian Naive Bayes model exhibited the lowest performance with an F1-Score (macro) of 0.6813, attributed to its strong assumption of feature independence.
*   For clustering, K-Means analysis suggested that **K=2** was the optimal number of clusters based on Silhouette and Davies-Bouldin metrics.

### Insights or Next Steps
*   **Deepen Optimization for Potential Models**: Conduct rigorous hyperparameter optimization (e.g., using GridSearchCV or RandomizedSearchCV) for models like MLP (including network architecture) and XGBoost to unlock their full potential.
*   **Continue Feature Engineering**: Given the significant impact of engineered features, further development and refinement of new features from existing data could yield additional performance improvements.
"""

# ------------------------------------------------------------
# Deployment model for real-time/demo interface (Streamlit)
# ------------------------------------------------------------

from sklearn.ensemble import RandomForestClassifier


if 'df_feat' in globals() and 'mental_wellness_index_0_100' in df_feat.columns:
    try:
        # Garantir que a função e os limites usados nas classes existem
        if 'categorize_wellness' in globals():
            df_deploy = df_feat.copy()
            df_deploy['mental_wellness_class'] = df_deploy['mental_wellness_index_0_100'].apply(categorize_wellness)

            # Conjunto de features simples que conseguimos pedir na interface
            candidate_features = [
                'screen_time_hours',
                'stress_level_0_10',
                'sleep_hours',
                'sleep_quality_1_5',
                'exercise_minutes_per_week',
                'productivity_0_100',
            ]
            deploy_features = [c for c in candidate_features if c in df_deploy.columns]

            if len(deploy_features) > 0:
                X_deploy = df_deploy[deploy_features]
                y_deploy = df_deploy['mental_wellness_class']

                rf_deploy = RandomForestClassifier(
                    n_estimators=200,
                    random_state=42,
                    max_depth=None,
                    min_samples_leaf=2,
                )
                rf_deploy.fit(X_deploy, y_deploy)

                artifact = {
                    'model': rf_deploy,
                    'feature_names': deploy_features,
                    'class_names': sorted(y_deploy.unique().tolist()),
                }

                dump(artifact, 'wellness_model.joblib')
                print("Modelo de deployment gravado em 'wellness_model.joblib'.")
                print('Features usadas no deployment:', deploy_features)
            else:
                print('Não foram encontradas features compatíveis para o modelo de deployment.')
        else:
            print('Função categorize_wellness não disponível — modelo de deployment não foi criado.')
    except Exception as e:
        print(f'Erro ao treinar/gravar o modelo de deployment: {e}')
else:
    print('df_feat não está disponível ou não contém a coluna mental_wellness_index_0_100. Modelo de deployment não foi criado.')


# ------------------------------------------------------------
# Exportar Data Mart para MySQL (fact_wellness)
# ------------------------------------------------------------

if 'df_feat' in globals() and 'mental_wellness_index_0_100' in df_feat.columns:
    try:
        if 'categorize_wellness' in globals():
            df_etl = df_feat.copy()
            df_etl['mental_wellness_class'] = df_etl['mental_wellness_index_0_100'].apply(categorize_wellness)

            # --------------------------------------------------------
            # Aplicar modelos finais para gerar colunas de Deployment
            #   - Regressão  → pred_wellness_index
            #   - Classificação → pred_wellness_class
            #   - Clustering → cluster_label
            # Estas colunas serão usadas depois em BI (Tableau).
            # --------------------------------------------------------

            try:
                # 1) Regressão: usar um RandomForestRegressor treinado em X_regression / y_regression
                if 'X_regression' in globals() and 'y_regression' in globals():
                    from sklearn.ensemble import RandomForestRegressor

                    rf_reg_final = RandomForestRegressor(
                        n_estimators=100,
                        random_state=42,
                    )
                    rf_reg_final.fit(X_regression, y_regression)
                    df_etl['pred_wellness_index'] = rf_reg_final.predict(X_regression)
                else:
                    print('X_regression / y_regression não disponíveis — coluna pred_wellness_index não foi criada.')

                # 2) Classificação: usar o melhor RandomForestClassifier otimizado (best_rf_classifier)
                if 'best_rf_classifier' in globals() and 'le' in globals() and 'X_classification' in globals():
                    y_pred_encoded = best_rf_classifier.predict(X_classification)
                    pred_classes = le.inverse_transform(y_pred_encoded)
                    df_etl['pred_wellness_class'] = pred_classes
                else:
                    print('Modelo de classificação ou codificador não disponíveis — coluna pred_wellness_class não foi criada.')

                # 3) Clustering: K-Means com K=2 (valor escolhido na análise)
                if 'X_classification' in globals():
                    from sklearn.cluster import KMeans

                    kmeans_final = KMeans(n_clusters=2, random_state=42, n_init=10)
                    cluster_labels_final = kmeans_final.fit_predict(X_classification)
                    df_etl['cluster_label'] = cluster_labels_final
                else:
                    print('Dados de classificação não disponíveis — coluna cluster_label não foi criada.')

            except Exception as e_pred:
                print(f'Erro ao gerar colunas de previsão para o Data Mart: {e_pred}')

            # PARAMETROS DA BASE DE DADOS (substituir pelo teu utilizador/password)
            USER = 'root'
            PASSWORD = 'aase2025'
            HOST = 'localhost'
            PORT = 3306
            DB = 'aase_wellness'

            try:
                engine = create_engine(f"mysql+pymysql://{USER}:{PASSWORD}@{HOST}:{PORT}/{DB}")
                df_etl.to_sql('fact_wellness', con=engine, if_exists='replace', index=False)
                print("Tabela 'fact_wellness' carregada na base de dados MySQL aase_wellness.")
                print('Número de registos exportados para MySQL:', len(df_etl))
            except Exception as db_err:
                print(f'Erro ao ligar/gravar em MySQL: {db_err}')
        else:
            print('Função categorize_wellness não disponível — Data Mart em MySQL não foi criado.')
    except Exception as e:
        print(f'Erro ao preparar dados para o Data Mart MySQL: {e}')
else:
    print('df_feat não está disponível ou não contém a coluna mental_wellness_index_0_100. Data Mart MySQL não foi criado.')
